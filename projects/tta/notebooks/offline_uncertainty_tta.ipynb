{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs01/home/abbasgln/codes/medAI/projects/tta\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "import medAI\n",
    "from medAI.utils.setup import BasicExperiment, BasicExperimentConfig\n",
    "\n",
    "from utils.metrics import MetricCalculator\n",
    "\n",
    "from timm.optim.optim_factory import create_optimizer\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "from datasets.datasets import ExactNCT2013RFImagePatches\n",
    "from medAI.datasets.nct2013 import (\n",
    "    KFoldCohortSelectionOptions,\n",
    "    LeaveOneCenterOutCohortSelectionOptions, \n",
    "    PatchOptions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAVE_OUT='JH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing positions: 100%|██████████| 616/616 [00:04<00:00, 139.55it/s]\n"
     ]
    }
   ],
   "source": [
    "###### No support dataset ######\n",
    "\n",
    "from ensemble_experiment import EnsembleConfig\n",
    "config = EnsembleConfig(cohort_selection_config=LeaveOneCenterOutCohortSelectionOptions(leave_out=f\"{LEAVE_OUT}\"),\n",
    "                        # patch_config=PatchOptions(needle_mask_threshold=0.6, prostate_mask_threshold=0.9, patch_size_mm = (3,3), strides = (1.2,1.2))\n",
    ")\n",
    "\n",
    "from baseline_experiment import BaselineConfig\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.tv_tensors import Image as TVImage\n",
    "\n",
    "class Transform:\n",
    "    def __init__(selfT, augment=False):\n",
    "        selfT.augment = augment\n",
    "        selfT.size = (256, 256)\n",
    "        # Augmentation\n",
    "        selfT.transform = T.Compose([\n",
    "            T.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "            T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0.5),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "        ])  \n",
    "    \n",
    "    def __call__(selfT, item):\n",
    "        patch = item.pop(\"patch\")\n",
    "        patch = copy(patch)\n",
    "        patch = (patch - patch.min()) / (patch.max() - patch.min()) \\\n",
    "            if config.instance_norm else patch\n",
    "        patch = TVImage(patch)\n",
    "        patch_orig_size = patch\n",
    "        patch = T.Resize(selfT.size, antialias=True)(patch).float()\n",
    "        \n",
    "        label = torch.tensor(item[\"grade\"] != \"Benign\").long()\n",
    "        \n",
    "        if selfT.augment:\n",
    "            patch_augs = torch.stack([selfT.transform(patch) for _ in range(2)], dim=0)\n",
    "            return patch_augs, patch, label, item #, patch_orig_size\n",
    "        \n",
    "        return -1, patch, label, item\n",
    "\n",
    "\n",
    "# val_ds_memo = ExactNCT2013RFImagePatches(\n",
    "#     split=\"val\",\n",
    "#     transform=Transform(augment=True),\n",
    "#     cohort_selection_options=config.cohort_selection_config,\n",
    "#     patch_options=config.patch_config,\n",
    "#     debug=config.debug,\n",
    "# )\n",
    "    \n",
    "if (config.cohort_selection_config.leave_out == \"UVA\"):\n",
    "    config.cohort_selection_config.benign_to_cancer_ratio = 5.0     \n",
    "\n",
    "test_ds_memo = ExactNCT2013RFImagePatches(\n",
    "    split=\"test\",\n",
    "    transform=Transform(augment=True),\n",
    "    cohort_selection_options=config.cohort_selection_config,\n",
    "    patch_options=config.patch_config,\n",
    "    debug=config.debug,\n",
    ")\n",
    "\n",
    "\n",
    "# val_loader_memo = DataLoader(\n",
    "#     val_ds_memo, batch_size=32, shuffle=True, num_workers=4\n",
    "# )\n",
    "test_loader_memo = DataLoader(\n",
    "    test_ds_memo, batch_size=32, shuffle=True, num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23214"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = test_ds_memo[0]\n",
    "len(test_ds_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader_memo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[-1].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(batch[1][:,32:64,:].permute(1, 2, 0).numpy(), aspect='auto')\n",
    "# plt.show()\n",
    "# plt.imshow(batch[-1][:,56:112,:].permute(1, 2, 0).numpy(), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_experiment import FeatureExtractorConfig\n",
    "\n",
    "fe_config = FeatureExtractorConfig()\n",
    "\n",
    "# Create the model\n",
    "list_models: tp.List[nn.Module] = [timm.create_model(\n",
    "    fe_config.model_name,\n",
    "    num_classes=fe_config.num_classes,\n",
    "    in_chans=1,\n",
    "    features_only=fe_config.features_only,\n",
    "    norm_layer=lambda channels: nn.GroupNorm(\n",
    "                    num_groups=fe_config.num_groups,\n",
    "                    num_channels=channels\n",
    "                    )) for _ in range(5)]\n",
    "\n",
    "CHECkPOINT_PATH = os.path.join(os.getcwd(), f'logs/tta/ensemble_5mdls_gn_3ratio_loco/ensemble_5mdls_gn_3ratio_loco_{LEAVE_OUT}/', 'best_model.ckpt')\n",
    "\n",
    "state = torch.load(CHECkPOINT_PATH)\n",
    "[model.load_state_dict(state[\"list_models\"][i]) for i, model in enumerate(list_models)]\n",
    "\n",
    "[model.eval() for model in list_models]\n",
    "[model.cuda() for model in list_models]\n",
    "\n",
    "a = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Turn requires_grad off for all layers except the last one\n",
    "# for model in list_models:\n",
    "#     for name, params in model.named_parameters():\n",
    "#         if name != \"fc.weight\" and name != \"fc.bias\":\n",
    "#                 params.requires_grad_(False)\n",
    "#                 # print(name)\n",
    "#                 # print(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempreture Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff80890ce0154625b4095775a11566c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = val_loader_memo\n",
    "\n",
    "metric_calculator = MetricCalculator()\n",
    "desc = \"val\"\n",
    "\n",
    "\n",
    "temp = torch.tensor(1.0).cuda().requires_grad_(True)\n",
    "beta = torch.tensor(0.0).cuda().requires_grad_(True)\n",
    "\n",
    "\n",
    "params = [temp, beta]\n",
    "_optimizer = optim.Adam(params, lr=1e-3)\n",
    "\n",
    "for epoch in range(1):\n",
    "    metric_calculator.reset()\n",
    "    for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "        images_augs, images, labels, meta_data = batch\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "\n",
    "        # Evaluate\n",
    "        with torch.no_grad():\n",
    "            stacked_logits = torch.stack([model(images) for model in list_models])\n",
    "        scaled_stacked_logits = stacked_logits/ temp + beta\n",
    "        losses = [nn.CrossEntropyLoss()(\n",
    "            scaled_stacked_logits[i, ...],\n",
    "            labels\n",
    "            ) for i in range(5)\n",
    "        ]\n",
    "        \n",
    "        # optimize\n",
    "        _optimizer.zero_grad()\n",
    "        sum(losses).backward()\n",
    "        _optimizer.step()\n",
    "                    \n",
    "        # Update metrics   \n",
    "        metric_calculator.update(\n",
    "            batch_meta_data = meta_data,\n",
    "            probs = F.softmax(scaled_stacked_logits, dim=-1).mean(dim=0).detach().cpu(), # Take mean over ensembles\n",
    "            labels = labels.detach().cpu(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.5950, device='cuda:0', requires_grad=True),\n",
       " tensor(-0.8514, device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JH\n",
    "# temp = torch.tensor(1.6793).cuda()\n",
    "# beta = torch.tensor(-1.0168).cuda()\n",
    "\n",
    "# PCC\n",
    "temp = torch.tensor(1.5950).cuda()\n",
    "beta = torch.tensor(-0.8514).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test MEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd489925d73340a094ef753e5a73324a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = test_loader_memo\n",
    "enable_memo = True\n",
    "certain_threshold = 0.8\n",
    "\n",
    "from memo_experiment import batched_marginal_entropy\n",
    "metric_calculator = MetricCalculator()\n",
    "desc = \"test\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "    images_augs, images, labels, meta_data = batch\n",
    "    images_augs = images_augs.cuda()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    adaptation_model_list = [deepcopy(model) for model in list_models] \n",
    "    [model.eval() for model in adaptation_model_list]\n",
    "    \n",
    "    if enable_memo:\n",
    "        batch_size, aug_size= images_augs.shape[0], images_augs.shape[1]\n",
    "\n",
    "        params = []\n",
    "        for model in adaptation_model_list:\n",
    "            params.append({'params': model.parameters()})\n",
    "        optimizer = optim.SGD(params, lr=5e-4)\n",
    "        \n",
    "        _images_augs = images_augs.reshape(-1, *images_augs.shape[2:]).cuda()\n",
    "        # Adapt to test\n",
    "        for j in range(1):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            stacked_logits = torch.stack([model(_images_augs).reshape(batch_size, aug_size, -1) for model in adaptation_model_list])\n",
    "            \n",
    "            # Remove uncertain samples from test-time adaptation\n",
    "            certain_idx = F.softmax(stacked_logits, dim=-1).mean(dim=0).mean(dim=1).max(dim=-1)[0] >= certain_threshold\n",
    "            stacked_logits = stacked_logits[:, certain_idx, ...]\n",
    "            \n",
    "            list_losses = []\n",
    "            list_logits = []\n",
    "            for k in range(5):\n",
    "                loss, logits = batched_marginal_entropy(stacked_logits[k,...])\n",
    "                list_losses.append(loss.mean())\n",
    "                list_logits.append(logits)\n",
    "            # Backward pass\n",
    "            sum(list_losses).backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    logits = torch.stack([model(images) for model in adaptation_model_list])\n",
    "    losses = [nn.CrossEntropyLoss()(\n",
    "        logits[i, ...],\n",
    "        labels\n",
    "        ) for i in range(5)\n",
    "    ]\n",
    "                    \n",
    "    # Update metrics   \n",
    "    metric_calculator.update(\n",
    "        batch_meta_data = meta_data,\n",
    "        probs = F.softmax(logits, dim=-1).mean(dim=0).detach().cpu(), # Take mean over ensembles\n",
    "        labels = labels.detach().cpu(),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b068e607074601b8ed8fd7e9d4d34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = test_loader_memo\n",
    "enable_memo = True\n",
    "\n",
    "from memo_experiment import batched_marginal_entropy\n",
    "metric_calculator = MetricCalculator()\n",
    "desc = \"test\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "    images_augs, images, labels, meta_data = batch\n",
    "    images_augs = images_augs.cuda()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    adaptation_model_list = [deepcopy(model) for model in list_models] \n",
    "    [model.eval() for model in adaptation_model_list]\n",
    "    \n",
    "    if enable_memo:\n",
    "        batch_size, aug_size= images_augs.shape[0], images_augs.shape[1]\n",
    "\n",
    "        params = []\n",
    "        for model in adaptation_model_list:\n",
    "            params.append({'params': model.parameters()})\n",
    "        optimizer = optim.SGD(params, lr=5e-4)\n",
    "        \n",
    "        _images_augs = images_augs.reshape(-1, *images_augs.shape[2:]).cuda()\n",
    "        # Adapt to test\n",
    "        for j in range(1):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            stacked_outputs = torch.stack([model(_images_augs).reshape(batch_size, aug_size, -1) for model in adaptation_model_list])\n",
    "            \n",
    "            # for outputs in len(adaptation_model_list):\n",
    "            #     loss, logits = batched_marginal_entropy(outputs)\n",
    "            #     list_losses.append(loss.mean())\n",
    "            #     list_logits.append(logits)\n",
    "            # # Backward pass\n",
    "            # sum(list_losses).backward()\n",
    "            # optimizer.step()\n",
    "            \n",
    "            loss, logits = batched_marginal_entropy(stacked_outputs.mean(dim=0))\n",
    "            # Backward pass\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    logits = torch.stack([model(images) for model in adaptation_model_list])\n",
    "    losses = [nn.CrossEntropyLoss()(\n",
    "        logits[i, ...],\n",
    "        labels\n",
    "        ) for i in range(5)\n",
    "    ]\n",
    "                    \n",
    "    # Update metrics   \n",
    "    metric_calculator.update(\n",
    "        batch_meta_data = meta_data,\n",
    "        probs = F.softmax(logits, dim=-1).mean(dim=0).detach().cpu(), # Take mean over ensembles\n",
    "        labels = labels.detach().cpu(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test/patch_auroc': tensor(0.6533),\n",
       " 'test/patch_accuracy': tensor(0.7716),\n",
       " 'test/all_inv_patch_auroc': tensor(0.6178),\n",
       " 'test/all_inv_patch_accuracy': tensor(0.7589),\n",
       " 'test/core_auroc': tensor(0.7585),\n",
       " 'test/core_accuracy': tensor(0.8788),\n",
       " 'test/all_inv_core_auroc': tensor(0.7104),\n",
       " 'test/all_inv_core_accuracy': tensor(0.8602)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_core_probs_first = False\n",
    "metric_calculator.avg_core_probs_first = avg_core_probs_first\n",
    "\n",
    "# Log metrics every epoch\n",
    "metrics = metric_calculator.get_metrics()\n",
    "\n",
    "# Update best score\n",
    "(best_score_updated,best_score) = metric_calculator.update_best_score(metrics, desc)\n",
    "\n",
    "best_score_updated = copy(best_score_updated)\n",
    "best_score = copy(best_score)\n",
    "        \n",
    "# Log metrics\n",
    "metrics_dict = {\n",
    "    f\"{desc}/{key}\": value for key, value in metrics.items()\n",
    "    }\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting test for poc of pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_indices = range(0, len(test_ds_memo) // 2)\n",
    "test_indices = range(len(test_ds_memo) // 2, len(test_ds_memo))\n",
    "\n",
    "# split test_ds_memo into two\n",
    "test_train = Subset(test_ds_memo, train_indices)\n",
    "test_test = Subset(test_ds_memo, test_indices)\n",
    "\n",
    "\n",
    "test_train_loader = DataLoader(\n",
    "    test_train, batch_size=64, shuffle=True, num_workers=4\n",
    ")\n",
    "test_test_loader = DataLoader(\n",
    "    test_test, batch_size=32, shuffle=False, num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0626143cb88e442ca249aa734338c247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/1990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loader = test_test_loader\n",
    "loader = test_loader_memo\n",
    "enable_pseudo_label = False\n",
    "temp_scale = False\n",
    "certain_threshold = 0.8\n",
    "\n",
    "metric_calculator = MetricCalculator()\n",
    "desc = \"test\"\n",
    "\n",
    "for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "    images_augs, images, labels, meta_data = batch\n",
    "    # images_augs = images_augs.cuda()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    adaptation_model_list = [deepcopy(model) for model in list_models] \n",
    "    [model.eval() for model in adaptation_model_list]\n",
    "\n",
    "    \n",
    "    if enable_pseudo_label:\n",
    "        params = []\n",
    "        for model in adaptation_model_list:\n",
    "            params.append({'params': model.parameters()})\n",
    "        optimizer = optim.SGD(params, lr=5e-4)\n",
    "        \n",
    "        # Adapt to test\n",
    "        for j in range(1):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            stacked_logits = torch.stack([model(images) for model in adaptation_model_list])\n",
    "            if temp_scale:\n",
    "                stacked_logits = stacked_logits / temp + beta\n",
    "            \n",
    "            # Remove uncertain samples from test-time adaptation\n",
    "            certain_idx = F.softmax(stacked_logits, dim=-1).mean(dim=0).max(dim=-1)[0] >= certain_threshold\n",
    "            stacked_logits = stacked_logits[:, certain_idx, ...]\n",
    "            \n",
    "            list_losses = []\n",
    "            for k, outputs in enumerate(adaptation_model_list):\n",
    "                loss = nn.CrossEntropyLoss()(stacked_logits[k, ...], F.softmax(stacked_logits, dim=-1).mean(dim=0).argmax(dim=-1))\n",
    "                list_losses.append(loss.mean())\n",
    "            # Backward pass\n",
    "            sum(list_losses).backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    # Evaluate\n",
    "    logits = torch.stack([model(images) for model in adaptation_model_list])\n",
    "    if temp_scale:\n",
    "        logits = logits / temp + beta\n",
    "    losses = [nn.CrossEntropyLoss()(\n",
    "        logits[i, ...],\n",
    "        labels\n",
    "        ) for i in range(5)\n",
    "    ]\n",
    "                    \n",
    "    # Update metrics   \n",
    "    metric_calculator.update(\n",
    "        batch_meta_data = meta_data,\n",
    "        probs = F.softmax(logits, dim=-1).mean(dim=0).detach().cpu(), # Take mean over ensembles\n",
    "        labels = labels.detach().cpu(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test/patch_auroc': tensor(0.6299),\n",
       " 'test/patch_accuracy': tensor(0.7978),\n",
       " 'test/all_inv_patch_auroc': tensor(0.6018),\n",
       " 'test/all_inv_patch_accuracy': tensor(0.7697),\n",
       " 'test/core_auroc': tensor(0.7187),\n",
       " 'test/core_accuracy': tensor(0.8981),\n",
       " 'test/all_inv_core_auroc': tensor(0.6690),\n",
       " 'test/all_inv_core_accuracy': tensor(0.8571)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_core_probs_first = True\n",
    "metric_calculator.avg_core_probs_first = avg_core_probs_first\n",
    "\n",
    "# Log metrics every epoch\n",
    "metrics = metric_calculator.get_metrics()\n",
    "\n",
    "# Update best score\n",
    "(best_score_updated,best_score) = metric_calculator.update_best_score(metrics, desc)\n",
    "\n",
    "best_score_updated = copy(best_score_updated)\n",
    "best_score = copy(best_score)\n",
    "        \n",
    "# Log metrics\n",
    "metrics_dict = {\n",
    "    f\"{desc}/{key}\": value for key, value in metrics.items()\n",
    "    }\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing 10% max probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## core probs and labels\n",
    "\n",
    "ids = metric_calculator.remove_low_inv_ids(metric_calculator.core_id_invs)\n",
    "# ids = list(metric_calculator.core_id_probs.keys())\n",
    "\n",
    "probs_avg_max = []\n",
    "for id, probs_list in metric_calculator.core_id_probs.items():\n",
    "    if id in ids:\n",
    "        core_len_10th = len(probs_list) // 5 + 1\n",
    "        probs, labels = torch.stack(probs_list).max(dim=1)\n",
    "        sorted_probs, indx = probs.sort()\n",
    "        sorted_probs = sorted_probs[-core_len_10th:]\n",
    "        indx = indx[-core_len_10th:]\n",
    "        labels_sorted_probs = labels[indx]\n",
    "        probs_avg_max.append(sum(sorted_probs*labels_sorted_probs)/core_len_10th)\n",
    "probs_avg_max = torch.stack(probs_avg_max)\n",
    "\n",
    "labels = torch.stack(\n",
    "    [labels_list[0] for id, labels_list in metric_calculator.core_id_labels.items() if id in ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6683), tensor(0.8987))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.functional.auroc(probs_avg_max, labels, task=\"binary\"), torchmetrics.functional.accuracy(probs_avg_max, labels, task=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WabdB Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/fs01/home/abbasgln/codes/medAI/projects/tta/wandb/run-20240212_103016-9acrlmo4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdigilany/tta/runs/9acrlmo4' target=\"_blank\">offline_combEnsmPsdo_avgprob_.8uncrtnty_gn_3ratio_loco_PCC</a></strong> to <a href='https://wandb.ai/mahdigilany/tta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdigilany/tta' target=\"_blank\">https://wandb.ai/mahdigilany/tta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdigilany/tta/runs/9acrlmo4' target=\"_blank\">https://wandb.ai/mahdigilany/tta/runs/9acrlmo4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mahdigilany/tta/runs/9acrlmo4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f83b8394880>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# group=f\"offline_combEnsmPsdo_.8uncrtnty_gn_3ratio_loco\"\n",
    "group=f\"offline_combEnsmPsdo_avgprob_.8uncrtnty_gn_3ratio_loco\"\n",
    "# group=f\"offline_ensemble_avgprob_5mdls_gn_3ratio_loco\"\n",
    "name= group + f\"_{LEAVE_OUT}\"\n",
    "wandb.init(project=\"tta\", entity=\"mahdigilany\", name=name, group=group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a891a90b1548399227a8fb9b2d5f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>test/all_inv_core_accuracy</td><td>▁</td></tr><tr><td>test/all_inv_core_auroc</td><td>▁</td></tr><tr><td>test/all_inv_patch_accuracy</td><td>▁</td></tr><tr><td>test/all_inv_patch_auroc</td><td>▁</td></tr><tr><td>test/core_accuracy</td><td>▁</td></tr><tr><td>test/core_auroc</td><td>▁</td></tr><tr><td>test/patch_accuracy</td><td>▁</td></tr><tr><td>test/patch_auroc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>test/all_inv_core_accuracy</td><td>0.85777</td></tr><tr><td>test/all_inv_core_auroc</td><td>0.67289</td></tr><tr><td>test/all_inv_patch_accuracy</td><td>0.79639</td></tr><tr><td>test/all_inv_patch_auroc</td><td>0.61083</td></tr><tr><td>test/core_accuracy</td><td>0.8994</td></tr><tr><td>test/core_auroc</td><td>0.7264</td></tr><tr><td>test/patch_accuracy</td><td>0.8278</td></tr><tr><td>test/patch_auroc</td><td>0.64418</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">offline_combEnsmPsdo_avgprob_.8uncrtnty_gn_3ratio_loco_PCC</strong> at: <a href='https://wandb.ai/mahdigilany/tta/runs/9acrlmo4' target=\"_blank\">https://wandb.ai/mahdigilany/tta/runs/9acrlmo4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240212_103016-9acrlmo4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_dict.update({\"epoch\": 0})\n",
    "wandb.log(\n",
    "    metrics_dict,\n",
    "    )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mttt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
